{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdbf4bceef377ee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "LangChain: Models, Prompts and Output Parsers\n",
    "Outline\n",
    "Direct API calls to OpenAI\n",
    "API calls through LangChain:\n",
    "Prompts\n",
    "Models\n",
    "Output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfeaa3b0dd6e0d50",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:18:55.352217459Z",
     "start_time": "2023-11-23T14:18:54.431727157Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d481028608745b03",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:34:12.871196844Z",
     "start_time": "2023-11-23T14:34:12.842188740Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY_THREE']\n",
    "# account for deprecation of LLM model\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "770a9b7ab1dccc0d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:24:25.507453680Z",
     "start_time": "2023-11-23T14:24:25.432637807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

     ]
    }
   ],
   "source": [
    "# !pip install python-dotenv\n",
    "# print(openai.api_key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d8e46a88048676",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:19:21.456750209Z",
     "start_time": "2023-11-23T14:19:21.416411200Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4aa06606d1b88deb"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc72e883440cfe9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:19:29.170341898Z",
     "start_time": "2023-11-23T14:19:27.050917020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'As an AI language model, I can tell you that the answer to 1+1 is 2.'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ed7f541e50cfdb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:11.409775387Z",
     "start_time": "2023-11-23T13:59:11.401989937Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acbb368ab6d217cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:11.481078006Z",
     "start_time": "2023-11-23T13:59:11.410809336Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dfced6291eb8ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:11.491610389Z",
     "start_time": "2023-11-23T13:59:11.455811940Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be17cbe4f5006fd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:15.510145647Z",
     "start_time": "2023-11-23T13:59:11.456366915Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(prompt)\n",
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcd5fec0c1bd51d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:15.514141291Z",
     "start_time": "2023-11-23T13:59:15.499347370Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie. To add to my frustration, the warranty doesn't cover the cost of cleaning up my kitchen. Can you please help me out, friend?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83663d8e35cf8c57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:15.544185838Z",
     "start_time": "2023-11-23T13:59:15.510805199Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad7de28176d000f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:15.545588527Z",
     "start_time": "2023-11-23T13:59:15.520402895Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "332fb8f4387f8e76",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:31:17.403184670Z",
     "start_time": "2023-11-23T14:31:17.368710526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_proxy='')"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat.openai_api_key = openai.api_key\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e41897f384e05d99",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:31:28.362503697Z",
     "start_time": "2023-11-23T14:31:28.316124420Z"
    }
   },
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8da7930bd5175956",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:31:31.299268763Z",
     "start_time": "2023-11-23T14:31:31.261138574Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a97d4bce4728f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:15.675442582Z",
     "start_time": "2023-11-23T13:59:15.615851811Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f9bae469adf3d62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:15.730530545Z",
     "start_time": "2023-11-23T13:59:15.616884424Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4405222e9f88a8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:15.731512518Z",
     "start_time": "2023-11-23T13:59:15.617677942Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0177d4cdf70494e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:31:39.747604103Z",
     "start_time": "2023-11-23T14:31:39.733015542Z"
    }
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e94dbf9425d3a398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:15.732895134Z",
     "start_time": "2023-11-23T13:59:15.618792187Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style,\n",
    "    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3afb65f9770f0518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:15.837976230Z",
     "start_time": "2023-11-23T13:59:15.619275211Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain.schema.messages.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9698d1fd81c780c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:15.839913844Z",
     "start_time": "2023-11-23T13:59:15.663957130Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\"\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "137769ebdcb08671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:16.056086951Z",
     "start_time": "2023-11-23T13:59:15.664573929Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Incorrect API key provided: sk-HJEFh***************************************Bpwl. You can find your API key at https://platform.openai.com/account/api-keys.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Call the LLM to translate to the style of the customer message\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m customer_response \u001B[38;5;241m=\u001B[39m \u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcustomer_messages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m customer_response\u001B[38;5;241m.\u001B[39mcontent\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/langchain/chat_models/base.py:600\u001B[0m, in \u001B[0;36mBaseChatModel.__call__\u001B[0;34m(self, messages, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    594\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    595\u001B[0m     messages: List[BaseMessage],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    598\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    599\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m--> 600\u001B[0m     generation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    603\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(generation, ChatGeneration):\n\u001B[1;32m    604\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m generation\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/langchain/chat_models/base.py:349\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[1;32m    347\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    348\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e)\n\u001B[0;32m--> 349\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    350\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    351\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)\n\u001B[1;32m    352\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    353\u001B[0m ]\n\u001B[1;32m    354\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/langchain/chat_models/base.py:339\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[1;32m    336\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    337\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    338\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 339\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    345\u001B[0m         )\n\u001B[1;32m    346\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    347\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/langchain/chat_models/base.py:492\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    490\u001B[0m     )\n\u001B[1;32m    491\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported:\n\u001B[0;32m--> 492\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    496\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/langchain/chat_models/openai.py:422\u001B[0m, in \u001B[0;36mChatOpenAI._generate\u001B[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[1;32m    420\u001B[0m message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[1;32m    421\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[0;32m--> 422\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletion_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    423\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessage_dicts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\n\u001B[1;32m    424\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/langchain/chat_models/openai.py:352\u001B[0m, in \u001B[0;36mChatOpenAI.completion_with_retry\u001B[0;34m(self, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    350\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 352\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_completion_with_retry\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/tenacity/__init__.py:289\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_f\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/tenacity/__init__.py:379\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    377\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 379\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/tenacity/__init__.py:314\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[0;34m(self, retry_state)\u001B[0m\n\u001B[1;32m    312\u001B[0m is_explicit_retry \u001B[38;5;241m=\u001B[39m fut\u001B[38;5;241m.\u001B[39mfailed \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fut\u001B[38;5;241m.\u001B[39mexception(), TryAgain)\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_explicit_retry \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry(retry_state)):\n\u001B[0;32m--> 314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfut\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter(retry_state)\n",
      "File \u001B[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    449\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 403\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    404\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    405\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    406\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/tenacity/__init__.py:382\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 382\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[1;32m    384\u001B[0m         retry_state\u001B[38;5;241m.\u001B[39mset_exception(sys\u001B[38;5;241m.\u001B[39mexc_info())  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/langchain/chat_models/openai.py:350\u001B[0m, in \u001B[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001B[0;34m(**kwargs)\u001B[0m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 350\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001B[0m, in \u001B[0;36mChatCompletion.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     27\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    139\u001B[0m ):\n\u001B[1;32m    140\u001B[0m     (\n\u001B[1;32m    141\u001B[0m         deployment_id,\n\u001B[1;32m    142\u001B[0m         engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    152\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    153\u001B[0m     )\n\u001B[0;32m--> 155\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    166\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[1;32m    167\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/openai/api_requestor.py:299\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    280\u001B[0m     method,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    287\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    288\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m    289\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[1;32m    290\u001B[0m         method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[1;32m    291\u001B[0m         url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    297\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[1;32m    298\u001B[0m     )\n\u001B[0;32m--> 299\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/openai/api_requestor.py:710\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[0;34m(self, result, stream)\u001B[0m\n\u001B[1;32m    702\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m    703\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response_line(\n\u001B[1;32m    704\u001B[0m             line, result\u001B[38;5;241m.\u001B[39mstatus_code, result\u001B[38;5;241m.\u001B[39mheaders, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    705\u001B[0m         )\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m parse_stream(result\u001B[38;5;241m.\u001B[39miter_lines())\n\u001B[1;32m    707\u001B[0m     ), \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    708\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    709\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 710\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    711\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    712\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    713\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    714\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    715\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    716\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    717\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/pythonChatGPTest/venv_new/lib/python3.10/site-packages/openai/api_requestor.py:775\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[0;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[1;32m    773\u001B[0m stream_error \u001B[38;5;241m=\u001B[39m stream \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mdata\n\u001B[1;32m    774\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[0;32m--> 775\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_error_response(\n\u001B[1;32m    776\u001B[0m         rbody, rcode, resp\u001B[38;5;241m.\u001B[39mdata, rheaders, stream_error\u001B[38;5;241m=\u001B[39mstream_error\n\u001B[1;32m    777\u001B[0m     )\n\u001B[1;32m    778\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[0;31mAuthenticationError\u001B[0m: Incorrect API key provided: sk-HJEFh***************************************Bpwl. You can find your API key at https://platform.openai.com/account/api-keys."
     ]
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)\n",
    "customer_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b0142e83b9230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:16.060946020Z",
     "start_time": "2023-11-23T13:59:16.058709593Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customer_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8b5948ec076a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:16.062885078Z",
     "start_time": "2023-11-23T13:59:16.062514610Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\"\n",
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbb83c9b2e4346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:59:16.163569519Z",
     "start_time": "2023-11-23T13:59:16.104266064Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b442393c25b96ba",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T13:59:16.104948954Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a9094eef995c3c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24c5de1dbcaae36a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:32:00.968903324Z",
     "start_time": "2023-11-23T14:32:00.951251212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"gift\": False,\n",
    "    \"delivery_days\": 5,\n",
    "    \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94931f91dd8feb2f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:32:03.180207584Z",
     "start_time": "2023-11-23T14:32:03.168875443Z"
    }
   },
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da07c34240f3ebac",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:32:07.799968527Z",
     "start_time": "2023-11-23T14:32:07.788514551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "Format the output as JSON with the following keys:\n",
      "gift\n",
      "delivery_days\n",
      "price_value\n",
      "\n",
      "text: {text}\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(review_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8b512b1c6222e84",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:32:13.031159555Z",
     "start_time": "2023-11-23T14:32:13.014795455Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "317c9bf55c7ba1c1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:33:55.684196297Z",
     "start_time": "2023-11-23T14:33:51.539517897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<class 'openai.api_resources.chat_completion.ChatCompletion'> model_name='gpt-3.5-turbo-0301' temperature=0.0 openai_api_key='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' openai_proxy=''\n",
      "content='{\\n    \"gift\": true,\\n    \"delivery_days\": 2,\\n    \"price_value\": [\"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"]\\n}'\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat.openai_api_key = openai.api_key\n",
    "print(chat)\n",
    "print()\n",
    "response = chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8a3ca52a1fc36a6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:34:43.418602780Z",
     "start_time": "2023-11-23T14:34:43.370321490Z"
    }
   },
   "outputs": [],
   "source": [
    "type(response.content)\n",
    "a = data = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68795e79e2cf7740",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:34:46.879999332Z",
     "start_time": "2023-11-23T14:34:46.864230519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# You will get an error by running this line of code \n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "print(a)\n",
    "print(a.get('gift'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4f96d1a84c3e2cd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:34:49.473595302Z",
     "start_time": "2023-11-23T14:34:49.460640677Z"
    }
   },
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema,\n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75c53ce5bffcc254",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:41:14.129010606Z",
     "start_time": "2023-11-23T14:41:14.090166645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e79027c283d377f4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:41:26.551741423Z",
     "start_time": "2023-11-23T14:41:26.486472049Z"
    }
   },
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96af24f4e950e381",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:41:29.900238809Z",
     "start_time": "2023-11-23T14:41:29.837455883Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review,\n",
    "                                  format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "198c6ab09ec98437",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:41:40.414647484Z",
     "start_time": "2023-11-23T14:41:40.377683433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfae6e2733d65f96",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:41:51.107674747Z",
     "start_time": "2023-11-23T14:41:46.509244989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:43:41.107918848Z",
     "start_time": "2023-11-23T14:43:41.052013384Z"
    }
   },
   "id": "1e6ebd9110135d6d"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "dict"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:43:50.884369434Z",
     "start_time": "2023-11-23T14:43:50.858838689Z"
    }
   },
   "id": "c636aa0bef21da8b"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "(True, '2')"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('gift'), output_dict.get('delivery_days')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:44:52.526093410Z",
     "start_time": "2023-11-23T14:44:52.502279656Z"
    }
   },
   "id": "9e3776eb0ec9984d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "69559ce82dbd1787"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
